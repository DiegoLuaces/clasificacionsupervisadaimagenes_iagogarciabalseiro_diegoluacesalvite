{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import Xception, MobileNetV3Large\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import drive\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Ruta y carga de datos\n",
    "ruta_base = '/content/drive/MyDrive/SCUBI DU'\n",
    "def is_valid_image(file_path):\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    return any(file_path.lower().endswith(ext) for ext in valid_extensions)\n",
    "\n",
    "clases_unicas = sorted([d for d in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, d))])\n",
    "class_to_label = {name: idx for idx, name in enumerate(clases_unicas)}\n",
    "\n",
    "imagenes_cargadas, etiquetas_cargadas = [], []\n",
    "for class_name in clases_unicas:\n",
    "    class_folder = os.path.join(ruta_base, class_name)\n",
    "    valid_images = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if is_valid_image(f)]\n",
    "    for image_path in valid_images:\n",
    "        img = load_img(image_path, target_size=(224, 224))\n",
    "        imagenes_cargadas.append(np.array(img))\n",
    "        etiquetas_cargadas.append(class_to_label[class_name])\n",
    "\n",
    "X = np.array(imagenes_cargadas) / 255.0\n",
    "y = to_categorical(np.array(etiquetas_cargadas))\n",
    "\n",
    "# Dividir en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# Parámetros óptimos fijos\n",
    "parametros_fijos = {\n",
    "    'batch_size': 6,\n",
    "    'activation': 'sigmoid',\n",
    "    'patience': 3,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 0.0001\n",
    "}\n",
    "\n",
    "# Configuraciones de capas\n",
    "layer_unit_configs = {\n",
    "    1: [[64], [128], [256]],\n",
    "    2: [[256, 128], [128, 64], [64, 32]],\n",
    "    3: [[256, 128, 64], [128, 64, 32], [64, 32, 16]]\n",
    "}\n",
    "\n",
    "# Valores de tolerancia\n",
    "modelos = {\n",
    "    'Xception': Xception,\n",
    "    'MobileNetV3Large': MobileNetV3Large\n",
    "}\n",
    "tolerancias = [0.005, 0.02]\n",
    "\n",
    "# Función para construir el modelo\n",
    "def build_model(base_model_fn, units_per_layer_list, activation='relu', learning_rate=0.001):\n",
    "    base_model = base_model_fn(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    for units in units_per_layer_list:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(y.shape[1], activation='softmax'))\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "resultados = []\n",
    "\n",
    "for nombre_modelo, modelo_base in modelos.items():\n",
    "    for tolerancia in tolerancias:\n",
    "        for num_layers, configs in layer_unit_configs.items():\n",
    "            for units_list in configs:\n",
    "                model = build_model(\n",
    "                    base_model_fn=modelo_base,\n",
    "                    units_per_layer_list=units_list,\n",
    "                    activation=parametros_fijos['activation'],\n",
    "                    learning_rate=parametros_fijos['learning_rate']\n",
    "                )\n",
    "                es = callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=parametros_fijos['patience'],\n",
    "                    min_delta=tolerancia,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    epochs=parametros_fijos['epochs'],\n",
    "                    batch_size=parametros_fijos['batch_size'],\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[es],\n",
    "                    verbose=0\n",
    "                )\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_true = np.argmax(y_test, axis=1)\n",
    "                y_pred_class = np.argmax(y_pred, axis=1)\n",
    "                f1_macro = f1_score(y_true, y_pred_class, average='macro')\n",
    "                f1_weighted = f1_score(y_true, y_pred_class, average='weighted')\n",
    "                precision = precision_score(y_true, y_pred_class, average='macro', zero_division=0)\n",
    "                recall = recall_score(y_true, y_pred_class, average='macro', zero_division=0)\n",
    "                resultados.append({\n",
    "                    'modelo': nombre_modelo,\n",
    "                    'tolerancia': tolerancia,\n",
    "                    'num_layers': num_layers,\n",
    "                    'units_per_layer': str(units_list),\n",
    "                    'f1_macro': f1_macro,\n",
    "                    'f1_weighted': f1_weighted,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall\n",
    "                })\n",
    "\n",
    "# Convertir a DataFrame y guardar\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados.to_excel(\"/content/resultados_comparacion_modelos_tolerancia.xlsx\", index=False)\n",
    "\n",
    "# Resumen de la mejor combinación por modelo\n",
    "top_config = df_resultados.sort_values(by='f1_macro', ascending=False).groupby('modelo').first().reset_index()\n",
    "top_config.to_excel(\"/content/top_combinaciones_f1.xlsx\", index=False)\n",
    "\n",
    "# Gráficas comparativas por modelo\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for metric in ['f1_macro', 'f1_weighted', 'precision', 'recall']:\n",
    "    g = sns.catplot(\n",
    "        data=df_resultados,\n",
    "        kind='bar',\n",
    "        x='num_layers',\n",
    "        y=metric,\n",
    "        hue='units_per_layer',\n",
    "        col='modelo',\n",
    "        height=6,\n",
    "        aspect=1.2\n",
    "    )\n",
    "    g.fig.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle(f'{metric.replace(\"_\", \" \").title()} por número de capas, estructura y tolerancia')\n",
    "    g.set_axis_labels(\"Nº de capas\", metric.replace(\"_\", \" \").title())\n",
    "    g._legend.set_title(\"Estructura de neuronas\")\n",
    "    plt.savefig(f\"/content/{metric}_comparacion_modelos_tolerancia.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Matriz de confusión para la mejor configuración por modelo\n",
    "for idx, row in top_config.iterrows():\n",
    "    modelo_nombre = row['modelo']\n",
    "    mejor_modelo_fn = modelos[modelo_nombre]\n",
    "    units = eval(row['units_per_layer'])\n",
    "    model = build_model(\n",
    "        base_model_fn=mejor_modelo_fn,\n",
    "        units_per_layer_list=units,\n",
    "        activation=parametros_fijos['activation'],\n",
    "        learning_rate=parametros_fijos['learning_rate']\n",
    "    )\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=parametros_fijos['patience'],\n",
    "        min_delta=row['tolerancia'],\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=parametros_fijos['epochs'],\n",
    "        batch_size=parametros_fijos['batch_size'],\n",
    "        validation_split=0.2,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred_class)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=clases_unicas, yticklabels=clases_unicas, cbar=True, square=True, linewidths=0)\n",
    "    plt.title(f\"Matriz de Confusión: {modelo_nombre}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/content/matriz_confusion_{modelo_nombre}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Exportar las mejores configuraciones detalladas\n",
    "top_config.to_excel(\"/content/mejores_resultados_detalle.xlsx\", index=False)\n",
    "print(\"Exportadas mejores combinaciones detalladas a: mejores_resultados_detalle.xlsx\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOI3a/7r0B48/NQFZWPWx5+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
