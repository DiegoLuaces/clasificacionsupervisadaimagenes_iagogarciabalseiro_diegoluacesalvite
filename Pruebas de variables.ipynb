{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 650948,
     "status": "ok",
     "timestamp": 1748539187076,
     "user": {
      "displayName": "Diego Luaces Alvite",
      "userId": "06020310494877465707"
     },
     "user_tz": -120
    },
    "id": "wkmdGA-lycrM",
    "outputId": "645f548d-2480-45de-c593-bb049e99e9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 12s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e653099b2e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e653099b2e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "Resultados exportados a Excel correctamente.\n"
     ]
    }
   ],
   "source": [
    "#Evaluación de Xception variando parámetros individuales, incluyendo tolerancia\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Ruta y carga de datos\n",
    "ruta_base = '/content/drive/MyDrive/SCUBI DU'\n",
    "def is_valid_image(file_path):\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    return any(file_path.lower().endswith(ext) for ext in valid_extensions)\n",
    "\n",
    "clases_unicas = sorted([d for d in os.listdir(ruta_base) if os.path.isdir(os.path.join(ruta_base, d))])\n",
    "class_to_label = {name: idx for idx, name in enumerate(clases_unicas)}\n",
    "\n",
    "imagenes_cargadas, etiquetas_cargadas = [], []\n",
    "for class_name in clases_unicas:\n",
    "    class_folder = os.path.join(ruta_base, class_name)\n",
    "    valid_images = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if is_valid_image(f)]\n",
    "    for image_path in valid_images:\n",
    "        img = load_img(image_path, target_size=(224, 224))\n",
    "        imagenes_cargadas.append(np.array(img))\n",
    "        etiquetas_cargadas.append(class_to_label[class_name])\n",
    "\n",
    "X = np.array(imagenes_cargadas) / 255.0\n",
    "y = to_categorical(np.array(etiquetas_cargadas))\n",
    "\n",
    "# Dividir en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# Valores por defecto y por testear\n",
    "parametros = {\n",
    "    'batch_size': {'valores': [2, 6, 12], 'por_defecto': 6},\n",
    "    'activation': {'valores': ['relu', 'tanh', 'sigmoid'], 'por_defecto': 'relu'},\n",
    "    'patience': {'valores': [3, 5, 10], 'por_defecto': 5},\n",
    "    'epochs': {'valores': [5, 10, 20], 'por_defecto': 10},\n",
    "    'dropout_layers': {'valores': [0, 1, 2], 'por_defecto': 1},\n",
    "    'learning_rate': {'valores': [0.0001, 0.001, 0.01], 'por_defecto': 0.001},\n",
    "    'tolerancia': {'valores': [0.1, 0.001, 0], 'por_defecto': 0}\n",
    "}\n",
    "\n",
    "# Crear modelo base con capas clasificadoras\n",
    "def build_model(activation='relu', dropout_layers=1, learning_rate=0.001):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    for _ in range(dropout_layers):\n",
    "        model.add(layers.Dense(256, activation=activation))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(y.shape[1], activation='softmax'))\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Almacenar resultados\n",
    "resultados = []\n",
    "\n",
    "# Bucle para cada parámetro\n",
    "for param, config in parametros.items():\n",
    "    for val in config['valores']:\n",
    "        # Setear parámetros actuales y por defecto\n",
    "        p = {\n",
    "            'batch_size': parametros['batch_size']['por_defecto'],\n",
    "            'activation': parametros['activation']['por_defecto'],\n",
    "            'patience': parametros['patience']['por_defecto'],\n",
    "            'epochs': parametros['epochs']['por_defecto'],\n",
    "            'dropout_layers': parametros['dropout_layers']['por_defecto'],\n",
    "            'learning_rate': parametros['learning_rate']['por_defecto'],\n",
    "            'tolerancia': parametros['tolerancia']['por_defecto']\n",
    "        }\n",
    "        p[param] = val\n",
    "\n",
    "        model = build_model(activation=p['activation'], dropout_layers=p['dropout_layers'], learning_rate=p['learning_rate'])\n",
    "        es = callbacks.EarlyStopping(monitor='val_loss', patience=p['patience'], min_delta=p['tolerancia'], restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=p['epochs'], batch_size=p['batch_size'], validation_split=0.2, callbacks=[es], verbose=0)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        f1s = f1_score(y_true, y_pred_class, average=None)\n",
    "        media = np.mean(f1s)\n",
    "        std = np.std(f1s)\n",
    "\n",
    "        resultado = {\n",
    "            'parametro': param,\n",
    "            'valor': val,\n",
    "            'f1_media': media,\n",
    "            'f1_std': std\n",
    "        }\n",
    "        for i, f1c in enumerate(f1s):\n",
    "            resultado[f'f1_clase_{i}'] = f1c\n",
    "\n",
    "        resultados.append(resultado)\n",
    "\n",
    "# Exportar a Excel\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados.to_excel(\"/content/resultados_xception_variables.xlsx\", index=False)\n",
    "print(\"Resultados exportados a Excel correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOI3a/7r0B48/NQFZWPWx5+",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
